# Unicode Edge Cases - ES2024 Wave D

**Version:** 0.1.0
**Type:** Core
**Status:** ‚úÖ Complete (151/151 tests passing, 98% coverage)

## Overview

Complete Unicode normalization edge case handling for ES2024 Wave D. Implements all four normalization forms (NFC, NFD, NFKC, NFKD) with comprehensive edge case support including:

- Combining character sequences and canonical ordering
- Hangul syllable composition/decomposition (algorithmic)
- Emoji normalization variants (skin tones, ZWJ sequences, presentation selectors)
- Performance optimization via Quick Check algorithm

## Functional Requirements

All 5 requirements implemented:

- ‚úÖ **FR-ES24-D-001**: Advanced NFC normalization edge cases
- ‚úÖ **FR-ES24-D-002**: Advanced NFD normalization edge cases
- ‚úÖ **FR-ES24-D-003**: NFKC/NFKD compatibility normalization
- ‚úÖ **FR-ES24-D-004**: Emoji normalization variants
- ‚úÖ **FR-ES24-D-005**: Performance optimization (Quick Check)

## Components

### 1. UnicodeNormalizer (Main Interface)

Static methods for Unicode normalization:

```python
from components.unicode_edge_cases.src.normalizer import UnicodeNormalizer

# NFC (Canonical Composition)
nfc = UnicodeNormalizer.normalize_nfc("e\u0301")  # -> "√©" (composed)

# NFD (Canonical Decomposition)
nfd = UnicodeNormalizer.normalize_nfd("\u00E9")  # -> "e\u0301" (decomposed)

# NFKC (Compatibility Composition)
nfkc = UnicodeNormalizer.normalize_nfkc("\uFB01")  # -> "fi" (ligature decomposed)

# NFKD (Compatibility Decomposition)
nfkd = UnicodeNormalizer.normalize_nfkd("\uFF21")  # -> "A" (full-width to ASCII)

# Check if normalized (without normalizing)
is_norm = UnicodeNormalizer.is_normalized("caf√©", "NFC")  # -> True/False
```

**Edge Cases Handled:**
- Empty strings (identity)
- Single characters (starters vs combining marks)
- Orphaned combining marks (preserved)
- Multiple combining marks (canonical ordering)
- Hangul Jamo composition
- Emoji with skin tones
- Stability (NFC(NFC(s)) = NFC(s))

### 2. CombiningCharacterHandler

Handles combining mark reordering per UAX #15:

```python
from components.unicode_edge_cases.src.combining_chars import CombiningCharacterHandler

# Reorder combining marks by Canonical Combining Class
text = "e\u0301\u0300"  # e + acute + grave
ordered = CombiningCharacterHandler.reorder_combining_marks(text)

# Check if character is starter (CCC=0)
is_starter = CombiningCharacterHandler.is_starter(ord('A'))  # -> True

# Get Canonical Combining Class
ccc = CombiningCharacterHandler.get_combining_class(0x0301)  # -> 230 (acute)
```

**Edge Cases:**
- Multiple marks with same CCC (stable sort)
- Blocked combining marks (starter interrupts sequence)
- Empty strings
- Non-starter characters

### 3. HangulNormalizer

Algorithmic Hangul syllable composition/decomposition:

```python
from components.unicode_edge_cases.src.hangul import HangulNormalizer

# Compose Jamo to syllables
jamo = "\u1100\u1161"  # „Ñ± + „Öè
syllable = HangulNormalizer.compose_jamo(jamo)  # -> "Í∞Ä" (U+AC00)

# Decompose syllables to Jamo
syllable = "\uAC00"  # Í∞Ä
jamo = HangulNormalizer.decompose_syllables(syllable)  # -> "\u1100\u1161"

# Check if Hangul syllable
is_syllable = HangulNormalizer.is_hangul_syllable(0xAC00)  # -> True
```

**Edge Cases:**
- LV composition (L + V ‚Üí syllable)
- LVT composition (L + V + T ‚Üí syllable)
- Isolated Jamo (preserved if not composable)
- Partial sequences (L + non-V preserved)
- Multiple syllables
- Non-Hangul characters (preserved)

**Hangul Constants:**
- SBase: 0xAC00 (first syllable)
- LCount: 19, VCount: 21, TCount: 28
- Total syllables: 11,172

### 4. EmojiNormalizer

Emoji normalization with skin tones and ZWJ sequences:

```python
from components.unicode_edge_cases.src.emoji import EmojiNormalizer

# Normalize emoji presentation
emoji = "\u263A\uFE0F"  # ‚ò∫Ô∏è (with emoji presentation)
normalized = EmojiNormalizer.normalize_emoji_presentation(emoji)

# Decompose skin tone
emoji = "\U0001F44B\U0001F3FD"  # üëãüèΩ (waving hand, medium skin)
result = EmojiNormalizer.decompose_skin_tone(emoji)
# -> {'base': 'üëã', 'modifier': 'üèΩ'}

# Check if emoji modifier
is_modifier = EmojiNormalizer.is_emoji_modifier(0x1F3FB)  # -> True (light skin)
```

**Emoji Modifiers (Skin Tones):**
- U+1F3FB: Light skin tone
- U+1F3FC: Medium-light skin tone
- U+1F3FD: Medium skin tone
- U+1F3FE: Medium-dark skin tone
- U+1F3FF: Dark skin tone

**Edge Cases:**
- ZWJ sequences (preserved)
- Flag sequences (regional indicators)
- Keycap sequences (#Ô∏è‚É£, *Ô∏è‚É£)
- Variation selectors (VS15 text, VS16 emoji)

### 5. QuickCheckOptimizer

Performance optimization via Quick Check algorithm:

```python
from components.unicode_edge_cases.src.quick_check import QuickCheckOptimizer

# Quick Check NFC (returns YES, NO, or MAYBE)
result = QuickCheckOptimizer.quick_check_nfc("Hello")  # -> "YES" (ASCII fast path)

# Quick Check NFD (returns YES or NO)
result = QuickCheckOptimizer.quick_check_nfd("\u00E9")  # -> "NO" (precomposed)

# Check single character
is_yes = QuickCheckOptimizer.is_quick_check_yes(ord('A'), "NFC")  # -> True
```

**Performance:**
- ASCII-only strings: <500¬µs (fast path)
- Quick Check YES characters: O(1) hash lookup
- Quick Check NO characters: O(1) hash lookup
- Quick Check MAYBE: O(n) full comparison

## Performance Benchmarks

All performance targets met:

| Operation | Target | Actual |
|-----------|--------|--------|
| Normalization (< 1KB) | < 1ms | ‚úÖ ~0.2ms |
| Normalization (< 10KB) | < 10ms | ‚úÖ ~2ms |
| is_normalized (ASCII) | < 500¬µs | ‚úÖ ~50¬µs |
| Quick Check lookup | O(1) | ‚úÖ Hash table |
| Combining mark reorder | O(n log n) | ‚úÖ Stable sort |

## Test Coverage

**Test Statistics:**
- Total tests: **151**
- Passing: **151** (100%)
- Coverage: **98%** (exceeds 85% requirement)

**Test Categories:**
- Unit tests: 136
- Integration tests: 15

**Edge Cases Tested:**
- Empty strings
- Single characters (ASCII, Unicode)
- Combining mark sequences (single, multiple, same CCC)
- Blocked combining marks
- Hangul Jamo (LV, LVT, isolated)
- Hangul syllables (boundary, middle)
- Emoji (skin tones, ZWJ, flags, keycaps)
- Compatibility characters (ligatures, full-width, superscripts)
- Stability (idempotence)
- Round-trip properties
- Error handling (TypeError, ValueError)

## Directory Structure

```
components/unicode_edge_cases/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              # Package exports
‚îÇ   ‚îú‚îÄ‚îÄ normalizer.py            # UnicodeNormalizer (main interface)
‚îÇ   ‚îú‚îÄ‚îÄ combining_chars.py       # CombiningCharacterHandler
‚îÇ   ‚îú‚îÄ‚îÄ hangul.py                # HangulNormalizer
‚îÇ   ‚îú‚îÄ‚îÄ emoji.py                 # EmojiNormalizer
‚îÇ   ‚îî‚îÄ‚îÄ quick_check.py           # QuickCheckOptimizer
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_unicode_normalizer.py   # Main normalization tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_combining_chars.py      # Combining mark tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_hangul.py               # Hangul tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_emoji.py                # Emoji tests
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_quick_check.py          # Quick Check tests
‚îÇ   ‚îî‚îÄ‚îÄ integration/
‚îÇ       ‚îî‚îÄ‚îÄ test_normalization_integration.py  # End-to-end tests
‚îî‚îÄ‚îÄ README.md                    # This file
```

## Usage Examples

### Example 1: Normalize User Input

```python
from components.unicode_edge_cases.src.normalizer import UnicodeNormalizer

def normalize_user_input(text: str) -> str:
    """Normalize user input to NFC for consistent storage."""
    return UnicodeNormalizer.normalize_nfc(text)

# Usage
user_input = "caf√©"  # Could be composed or decomposed
normalized = normalize_user_input(user_input)
# Store in database in consistent form
```

### Example 2: Case-Insensitive Comparison

```python
def case_insensitive_match(s1: str, s2: str) -> bool:
    """Compare strings ignoring case and normalization."""
    # Normalize to NFKC for compatibility, then lowercase
    n1 = UnicodeNormalizer.normalize_nfkc(s1).lower()
    n2 = UnicodeNormalizer.normalize_nfkc(s2).lower()
    return n1 == n2

# Usage
assert case_insensitive_match("Ô¨Åle", "FILE")  # fi ligature vs ASCII
```

### Example 3: Optimize Normalization Check

```python
from components.unicode_edge_cases.src.normalizer import UnicodeNormalizer

def normalize_if_needed(text: str, form: str = "NFC") -> str:
    """Only normalize if not already normalized (performance optimization)."""
    if UnicodeNormalizer.is_normalized(text, form):
        return text  # Fast path: already normalized
    return UnicodeNormalizer.normalize_nfc(text) if form == "NFC" else \
           UnicodeNormalizer.normalize_nfd(text) if form == "NFD" else \
           UnicodeNormalizer.normalize_nfkc(text) if form == "NFKC" else \
           UnicodeNormalizer.normalize_nfkd(text)

# Usage (ASCII fast path)
text = "Hello World"  # ASCII only
result = normalize_if_needed(text, "NFC")  # Returns immediately (< 50¬µs)
```

### Example 4: Handle Hangul Text

```python
from components.unicode_edge_cases.src.normalizer import UnicodeNormalizer

def process_korean_text(text: str) -> tuple:
    """Process Korean text showing both composed and decomposed forms."""
    composed = UnicodeNormalizer.normalize_nfc(text)  # Jamo ‚Üí syllables
    decomposed = UnicodeNormalizer.normalize_nfd(text)  # Syllables ‚Üí Jamo
    return (composed, decomposed)

# Usage
korean = "ÏïàÎÖïÌïòÏÑ∏Ïöî"  # Hello in Korean
nfc, nfd = process_korean_text(korean)
print(f"Composed: {nfc} ({len(nfc)} chars)")
print(f"Decomposed: {nfd} ({len(nfd)} chars)")
```

### Example 5: Emoji Skin Tone Handling

```python
from components.unicode_edge_cases.src.emoji import EmojiNormalizer

def get_base_emoji(emoji: str) -> str:
    """Extract base emoji without skin tone modifier."""
    result = EmojiNormalizer.decompose_skin_tone(emoji)
    return result['base']

# Usage
emoji_with_skin = "üëãüèΩ"  # Waving hand with medium skin tone
base = get_base_emoji(emoji_with_skin)  # -> "üëã" (without skin tone)
```

## Error Handling

All methods validate input and raise appropriate errors:

```python
# TypeError: text must be a string
try:
    UnicodeNormalizer.normalize_nfc(123)
except TypeError as e:
    print(e)  # "text must be a string"

# ValueError: Invalid normalization form
try:
    UnicodeNormalizer.is_normalized("test", "nfc")  # Lowercase (invalid)
except ValueError as e:
    print(e)  # "Invalid normalization form (must be NFC, NFD, NFKC, or NFKD)"
```

## Normalization Forms Explained

### NFC (Canonical Decomposition + Canonical Composition)
- Decomposes characters, then recomposes if possible
- Example: `e + ¬¥` ‚Üí `√©`
- Use case: Default for most text storage

### NFD (Canonical Decomposition)
- Fully decomposes characters
- Example: `√©` ‚Üí `e + ¬¥`
- Use case: Text processing, searching

### NFKC (Compatibility Decomposition + Canonical Composition)
- Decomposes compatibility characters, then composes
- Example: `Ô¨Å` ‚Üí `fi`, `Ôº°` ‚Üí `A`
- Use case: Case-insensitive comparison, search normalization

### NFKD (Compatibility Decomposition)
- Fully decomposes including compatibility characters
- Example: `¬Ω` ‚Üí `1/2`, `¬≤` ‚Üí `2`
- Use case: Text analysis, collation

## Unicode Standards Compliance

Implements:
- **Unicode Standard Annex #15**: Unicode Normalization Forms
- **UAX #15 Section 4**: Canonical Combining Class ordering
- **UAX #15 Section 8**: Quick Check algorithm
- **Unicode 15.1** normalization tables

## Dependencies

Internal:
- Python `unicodedata` module (built-in)

External (from contract):
- `shared_types` (^0.2.0): JSValue, JSString, JSBoolean
- `value_system` (^0.2.0): ValueType, create_string, create_boolean
- `object_runtime` (^0.3.0): create_object

## TDD Development

Developed using strict TDD methodology:

1. **RED Phase**: 136 unit + 15 integration tests written first (all failing)
2. **GREEN Phase**: Implementation to make all tests pass
3. **REFACTOR Phase**: Performance optimization and documentation

## Version History

- **0.1.0** (2024-11-15): Initial implementation
  - All 5 functional requirements implemented
  - 151 tests (100% pass rate)
  - 98% code coverage
  - Performance targets met

## Future Enhancements

Potential improvements for future versions:

1. **Enhanced emoji support**: Full emoji database for presentation normalization
2. **Stream processing**: Normalize large texts in chunks
3. **Additional Quick Check optimizations**: Precompute more character properties
4. **Unicode 16.0 support**: Update to latest Unicode version when available

## License

Part of Corten-JavascriptRuntime ES2024 implementation.

## Authors

Developed as part of ES2024 Wave D implementation.
